<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en-us" lang="en-us">
<head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="generator" content="Hugo 0.59.1" />

  <title>Troubleshooting performance of VDO and NFS &middot; Calgary RHCE</title>

  <meta name="description" content="" />

  
  <meta property="og:locale" content="en-us"/>

  
  <meta property="og:image" content="https://aludwar.github.iowp-content/uploads/2016/08/cropped-1477782_10152102853181489_71278912_n.jpg">

  
  <meta property="og:site_name" content="Calgary RHCE"/>
  <meta property="og:title" content="Troubleshooting performance of VDO and NFS"/>
  <meta property="og:description" content="In setting up a local virtualization environment a little while back, I thought I&#8217;d try the recently GA&#8217;d VDO capabilities in the RHEL 7.5 kernel. These include data compression and de-duplication natively in the linux kernel (through a kernel module). This was Red Hat&#8217;s efforts behind the Permabit acquisition. Considering a virtualization data store is a prime candidate for a de-duplication use-case, I was anxious to reclaim some of my storage budget ðŸ™‚ ."/>
  <meta property="og:url" content="https://aludwar.github.io/blog/2018/10/20/troubleshooting-performance-of-vdo-and-nfs/"/>
  <meta property="og:type" content="article"/>
  <meta property="article:published_time" content="2018-10-20T17:43:07Z"/>
  <meta property="article:modified_time" content="2018-10-20T17:43:07Z"/>
  <meta property="article:author" content="Andrew Ludwar">
  
  
  

  <script type="application/ld+json">
  {
    "@context" : "http://schema.org",
    "@type" : "Blog",
    "name": "Calgary RHCE",
    "url" : "https://aludwar.github.io",
    "image": "https://aludwar.github.iowp-content/uploads/2016/08/cropped-1477782_10152102853181489_71278912_n.jpg",
    "description": "A linux and open source technology blog."
  }
  </script>

  
  <script type="application/ld+json">
  {
    "@context": "http://schema.org",
    "@type": "BlogPosting",
    "name": "Troubleshooting performance of VDO and NFS",
    "headline": "Troubleshooting performance of VDO and NFS",
    "datePublished": "2018-10-20T17:43:07Z",
    "dateModified": "2018-10-20T17:43:07Z",
    "author": {
      "@type": "Person",
      "name": "Andrew Ludwar",
      "url": "https://aludwar.github.io"
    },
    "image": "https://aludwar.github.iowp-content/uploads/2016/08/cropped-1477782_10152102853181489_71278912_n.jpg",
    "url": "https://aludwar.github.io/blog/2018/10/20/troubleshooting-performance-of-vdo-and-nfs/",
    "description": "In setting up a local virtualization environment a little while back, I thought I\x26#8217;d try the recently GA\x26#8217;d VDO capabilities in the RHEL 7.5 kernel. These include data compression and de-duplication natively in the linux kernel (through a kernel module). This was Red Hat\x26#8217;s efforts behind the Permabit acquisition. Considering a virtualization data store is a prime candidate for a de-duplication use-case, I was anxious to reclaim some of my storage budget ðŸ™‚ ."
  }
  </script>
  


  <link type="text/css"
        rel="stylesheet"
        href="https://aludwar.github.iocss/print.css"
        media="print">

  <link type="text/css"
        rel="stylesheet"
        href="https://aludwar.github.iocss/poole.css">

  <link type="text/css"
        rel="stylesheet"
        href="https://aludwar.github.iocss/hyde.css">

  
<style type="text/css">
  .sidebar {
    background-color: #990000;
  }

  .read-more-link a {
    border-color: #990000;
  }

  footer a,
  .content a,
  .related-posts li a:hover {
    color: #990000;
  }
</style>



  

  <link rel="stylesheet"
        href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700&display=swap">

  <link rel="stylesheet"
        href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css"
        integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk="
        crossorigin="anonymous" />

  <link rel="apple-touch-icon-precomposed"
        sizes="144x144"
        href="/apple-touch-icon-144-precomposed.png">

  <link rel="shortcut icon" href="/favicon.png">

  
  </head>
<body>
  <aside class="sidebar">
  <div class="container">
    <div class="sidebar-about">
      
        
        <div class="author-image">
          <img src="https://aludwar.github.iowp-content/uploads/2016/08/cropped-1477782_10152102853181489_71278912_n.jpg" class="img-circle img-headshot center" alt="Profile Picture">
        </div>
        
      

      <h1>Calgary RHCE</h1>

      
      <p class="lead">A linux and open source technology blog.</p>
      
    </div>

    <nav>
      <ul class="sidebar-nav">
        <li>
          <a href="https://aludwar.github.io">Home</a>
        </li>
        <li>
          <a href="/posts/"> Posts </a>
        </li><li>
          <a href="/about/"> About </a>
        </li>
      </ul>
    </nav>

    <section class="social-icons">
      
      <a href="https://ca.linkedin.com/in/andrewludwar" rel="me" title="Linkedin">
        <i class="fab fa-linkedin" aria-hidden="true"></i>
      </a>
      
      <a href="https://github.com/aludwar" rel="me" title="GitHub">
        <i class="fab fa-github" aria-hidden="true"></i>
      </a>
      
      <a href="https://twitter.com/andrewludwar" rel="me" title="Twitter">
        <i class="fab fa-twitter" aria-hidden="true"></i>
      </a>
      
    </section>
  </div>
</aside>


  <main class="content container">
  <div class="post">
  <h1>Troubleshooting performance of VDO and NFS</h1>

  <div class="post-date">
    <time datetime="2018-10-20T17:43:07Z">Oct 20, 2018</time> Â· 7 min read
  </div>

  <p>In setting up a local virtualization environment a little while back, I thought I&#8217;d try the <a href="https://www.redhat.com/en/blog/look-vdo-new-linux-compression-layer">recently GA&#8217;d VDO capabilities</a> in the RHEL 7.5 kernel. These include data compression and de-duplication natively in the linux kernel (through a kernel module). This was Red Hat&#8217;s efforts behind the Permabit acquisition. Considering a virtualization data store is a prime candidate for a de-duplication use-case, I was anxious to reclaim some of my storage budget ðŸ™‚ . I was also curious to see what the extra overhead was like (if any), and understand the general performance characteristics of VDO.</p>

<p>I found VDO quite easy to setup, <a href="https://rhelblog.redhat.com/2018/04/17/how-to-set-up-a-rhel-nfs-server-with-vdo-data-reduction/">I followed this guide</a> basically verbatim. Given my NFS virtualization back-end stores mostly the same OS images, I was happy to see excellent deduplication stats on my VDO device:</p>

<pre class="">[root@nfs vms]# vdostats --si
Device                    Size      Used Available Use% Space saving%
/dev/mapper/vdo0          2.5T    239.3G      2.3T   9%           60%</pre>

<p>After a few months and several VMs spawned later, I noticed some slowness whenever I was doing high IO work like database updates and copying several gigs of data to disks all at once. (When my Satellite server downloads 50+ GB for a new content repo, and index&#8217;s it for example). My other VMs would notice a bit of a slowdown during this time. Given I hadn&#8217;t done much for tuning in this environment, it was probably time to look into it. I&#8217;ve also been debating upgrading my home lab to 10G networking and this seemed to line up with what I was seeing for storage performance. I thought I finally was being bottlenecked by the network, given I&#8217;ve got an SSD array in my NFS server, with a 4-port 1GB NIC in LACP. But before I went crazy buying 10G networking gear, I looked at tuning what I had.</p>

<p>The <a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/storage_administration_guide/vdo-ig-tuning-vdo">official documentation</a> is fairly good at explaining the performance characteristics of VDO, and what you might want to tune. I also went into NFS server/client tuning as I hadn&#8217;t done much for this either. Given there&#8217;s a few things at play here (disk hardware performance, network performance, VDO optimization, NFS optimization) I quickly went down a few rabbit holes and realised I needed to do some basic benchmarking and baselining so I could understand which areas in this stack were actually performing well, and which ones were candidates for more tuning. In addition to the VDO tuning docs, here&#8217;s what I used for reference:</p>

<ul>
<li><a href="https://access.redhat.com/solutions/2216">How to increase the number of threads created by the NFS daemon in RHEL 4, 5 and 6?</a></li>
<li><a href="https://access.redhat.com/articles/333973">How can I improve the performance of my RHEL NFS server?</a></li>
<li><a href="https://access.redhat.com/articles/1172303">Initial baseline data collection for NFS client streaming I/O performance</a></li>
<li><a href="https://access.redhat.com/solutions/415263">High I/O wait to NFS share on NAS</a></li>
<li><a href="https://access.redhat.com/solutions/21301">RHEL network interface dropping packets</a></li>
</ul>

<p>Firstly, it&#8217;s important to troubleshoot things in isolation, and use a benchmarking method that&#8217;s complimentary to isolation as well. I used the iperf3 utility for network benchmarking and fio utility for disk benchmarking. With this I&#8217;d do a series of sequential read, sequential write, random read, and random read and write tests, and these would be done both on local disk filesystems and over the network filesystem. For reference, here&#8217;s the fio commands:</p>

<pre class=""># Sequential read
# fio --name TEST --eta-newline=5s --filename=fio-tempfile.dat --rw=read --size=500m --io_size=10g --blocksize=1024k --ioengine=libaio --fsync=10000 --iodepth=32 --direct=1 --numjobs=1 --runtime=60 --group_reporting

# Sequential write
# fio --name TEST --eta-newline=5s --filename=fio-tempfile.dat --rw=write --size=500m --io_size=10g --blocksize=1024k --ioengine=libaio --fsync=10000 --iodepth=32 --direct=1 --numjobs=1 --runtime=60 --group_reporting

# Random read
# fio --name TEST --eta-newline=5s --filename=fio-tempfile.dat --rw=randread --size=500m --io_size=10g --blocksize=4k --ioengine=libaio --fsync=1 --iodepth=1 --direct=1 --numjobs=1 --runtime=60 --group_reporting

# Random read and write
# fio --name TEST --eta-newline=5s --filename=fio-tempfile.dat --rw=randrw --size=500m --io_size=10g --blocksize=4k --ioengine=libaio --fsync=1 --iodepth=1 --direct=1 --numjobs=1 --runtime=60 --group_reporting<code></code></pre>

<p>After reading the above guides and doing some basic investigating and benchmarking, this is what I ended up tuning first:</p>

<p>Overall the networking looked alright. I saw some dropped packets, but I&#8217;ve been doing a fair amount of cable pulling, stop/starting hosts, and VPN up/down. The NICs on NFS server and clients weren&#8217;t using their full ring buffer, so I changed this. I don&#8217;t think this was much of a candidate for the dropped packets, but this tuning couldn&#8217;t hurt.</p>

<pre class="">[root@nfs]# ethtool -g eno1
Ring parameters for eno1:
Pre-set maximums:
RX: 2047
RX Mini: 0
RX Jumbo: 0
TX: 511
Current hardware settings:
RX: 200
RX Mini: 0
RX Jumbo: 0
TX: 511

# ethtool -G eno1 rx 2047
# ethtool -G eno2 rx 2047
# ethtool -G eno3 rx 2047
# ethtool -G eno4 rx 2047

# ethtool -g eno1
Ring parameters for eno1:
Pre-set maximums:
RX: 2047
RX Mini: 0
RX Jumbo: 0
TX: 511
Current hardware settings:
RX: 2047
RX Mini: 0
RX Jumbo: 0
TX: 511


[root@curie]# ethtool -g enp1s0
Ring parameters for enp1s0:
Pre-set maximums:
RX: 511
RX Mini: 0
RX Jumbo: 0
TX: 511
Current hardware settings:
RX: 200
RX Mini: 0
RX Jumbo: 0
TX: 511

# ethtool -G enp2s0 rx 511
# ethtool -G enp1s0 rx 511

# ethtool -g enp1s0
Ring parameters for enp1s0:
Pre-set maximums:
RX: 511
RX Mini: 0
RX Jumbo: 0
TX: 511
Current hardware settings:
RX: 511
RX Mini: 0
RX Jumbo: 0
TX: 511</pre>

<p>I also increased the default number of NFS threads on the NFS server. Considering I&#8217;ve got 15+ VMs, each VM looks to use 2-3 nfs threads depending on number of disks, tuning the default of 8 to 20 should help for concurrent disk activity:</p>

<pre class=""># egrep COUNT /etc/sysconfig/nfs
RPCNFSDCOUNT=20</pre>

<p>Similarly, the VDO device I created only had 1 thread allocated in several places, so I upped these as well and doubled the cache size:</p>

<pre class="">vdo modify --all --vdoLogicalThreads=4 --vdoPhysicalThreads=4 --vdoBioThreads=6 --vdoCpuThreads=6 --vdoAckThreads=2 --blockMapCacheSize=256M</pre>

<p>After these changes, I still wasn&#8217;t seeing any significant performance change. I was getting fairly abysmal speeds even on a local SSD filesystem on the NFS server, not even going over the network. I started to isolate this, and started to suspect a hardware/SSD tuning related issue. After updating my DL360p 420i storage controller firmware, making sure the RAID controller cache was disabled, <a href="https://kallesplayground.wordpress.com/useful-stuff/hp-smart-array-cli-commands-under-esxi/">SSD smart path was on</a>, it started to dawn on me. Previously, these six SSD drives had been used in a RAID5 configuration that saw a ton of heavy disk IO. I had dedicated this host to an OpenStack environment and had done several builds hammering these disks. RAID5 is not an optimal SSD configuration, parity calculation is expensive, and this would add a ton of disk IO and disk wear that wouldn&#8217;t be present in a RAID0 or RAID10 configuration. Essentially, I&#8217;ve got worn SSDs. I needed to do a <a href="https://grok.lsu.edu/Article.aspx?articleid=16716">secure erase of these SSDs</a> to return their cells to as close to original factory condition as one could get. These disks are approx 3 years old and haven&#8217;t had this done yet.</p>

<p>After doing an enhanced secure erase, I saw my local disk storage speeds come back up about 5 fold. This was more in line with the newer SSDs in other servers. Doing disk tests over the network saw the same speed increase. So it looks like my problem was entirely hardware related :). As I&#8217;m now turning on all the VMs, I&#8217;m seeing a much quicker response when doing the high IO activities. There&#8217;s more than 16 NFS threads consistently in use now and I&#8217;m monitoring to see the change in VDO related performance. I need to research a utility to get accurate VDO stats, I think this likely will be with a PCP module. But at first glance, with not much concrete data to go on yet, I *think* the VDO tuning has helped as well.</p>

<p>While I learned a bit about VDO and NFS performance tuning, it looks like I might need to spend that 10G networking budget on new SSDs instead. There&#8217;s diminishing life left in these.</p>

</div>


  </main>

  <footer>
  <div class="copyright">
    &copy;  2020 Â· 
  </div>
</footer>


  <script src="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/js/all.min.js"
          integrity="sha256-MAgcygDRahs+F/Nk5Vz387whB4kSK9NXlDN3w58LLq0="
          crossorigin="anonymous"></script>

  

  
</body>
</html>
