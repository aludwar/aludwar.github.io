<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>storage on Calgary RHCE</title>
    <link>https://calgaryrhce.ca/categories/storage/</link>
    <description>Recent content in storage on Calgary RHCE</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 22 Jun 2019 16:09:45 +0000</lastBuildDate><atom:link href="https://calgaryrhce.ca/categories/storage/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>InSync Google Drive client in Fedora 30</title>
      <link>https://calgaryrhce.ca/blog/2019/06/22/insync-google-drive-client-in-fedora-30/</link>
      <pubDate>Sat, 22 Jun 2019 16:09:45 +0000</pubDate>
      
      <guid>https://calgaryrhce.ca/blog/2019/06/22/insync-google-drive-client-in-fedora-30/</guid>
      <description>After dropbox ended its support for linux clients, I went looking for a different cloud storage solution to sync data between my linux systems. I ended up purchasing 100GB with Google Drive (now Google One) and also purchasing a linux client that would connect to Google Drive and sync data between the cloud and my linux systems.</description>
    </item>
    
    <item>
      <title>Troubleshooting performance of VDO and NFS</title>
      <link>https://calgaryrhce.ca/blog/2018/10/20/troubleshooting-performance-of-vdo-and-nfs/</link>
      <pubDate>Sat, 20 Oct 2018 17:43:07 +0000</pubDate>
      
      <guid>https://calgaryrhce.ca/blog/2018/10/20/troubleshooting-performance-of-vdo-and-nfs/</guid>
      <description>In setting up a local virtualization environment a little while back, I thought Iâ€™d try the recently GAâ€™d VDO capabilities in the RHEL 7.5 kernel. These include data compression and de-duplication natively in the linux kernel (through a kernel module). This was Red Hatâ€™s efforts behind the Permabit acquisition. Considering a virtualization data store is a prime candidate for a de-duplication use-case, I was anxious to reclaim some of my storage budget ðŸ™‚ .</description>
    </item>
    
    <item>
      <title>Migrating RHEV storage domains</title>
      <link>https://calgaryrhce.ca/blog/2018/10/19/migrating-rhev-storage-domains/</link>
      <pubDate>Fri, 19 Oct 2018 21:20:50 +0000</pubDate>
      
      <guid>https://calgaryrhce.ca/blog/2018/10/19/migrating-rhev-storage-domains/</guid>
      <description>Recently, Iâ€™ve been doing some troubleshooting in my virtualization environment, specifically with the NFS storage backing it. To isolate an issue I needed to migrate all the VM disks off the main data store to another one. I hadnâ€™t performed this kind of activity before, but found it to be quite easy.</description>
    </item>
    
    <item>
      <title>Filesystem benchmarking</title>
      <link>https://calgaryrhce.ca/blog/2016/09/11/filesystem-benchmarking/</link>
      <pubDate>Mon, 12 Sep 2016 02:51:38 +0000</pubDate>
      
      <guid>https://calgaryrhce.ca/blog/2016/09/11/filesystem-benchmarking/</guid>
      <description>Once in a while I need to do some I/O benchmarking. Either when creating a baseline for a before and after performance tuning comparison, or just because I want to see how fast my new SSD/M.2 really is. The tool I find myself going to over the years has been iozone.</description>
    </item>
    
    <item>
      <title>Reliable, resilient storage with GlusterFS</title>
      <link>https://calgaryrhce.ca/blog/2014/08/18/reliable-resilient-storage-with-glusterfs/</link>
      <pubDate>Tue, 19 Aug 2014 05:42:26 +0000</pubDate>
      
      <guid>https://calgaryrhce.ca/blog/2014/08/18/reliable-resilient-storage-with-glusterfs/</guid>
      <description>A need came up lately for some inexpensive resilient storage that was easily expandable, and that spanned multiple datacentres.Â Having recently been playing with GlusterFS and Swift in our OpenStack trial, I was quick to point out that these were strong use-cases for a GlusterFS architecture. We needed something right away, and something that also wasnâ€™t terribly expensive, which Gluster caters to quite well.</description>
    </item>
    
    <item>
      <title>Performance Tweaking â€“ SSDs, I/O Scheduler, Virtual Memory, Browser Cache</title>
      <link>https://calgaryrhce.ca/blog/2014/07/13/performance-tweaking-ssds-io-scheduler-virtual-memory-browser-cache/</link>
      <pubDate>Sun, 13 Jul 2014 22:15:31 +0000</pubDate>
      
      <guid>https://calgaryrhce.ca/blog/2014/07/13/performance-tweaking-ssds-io-scheduler-virtual-memory-browser-cache/</guid>
      <description>Having recently been exposed to some SSD tweaking at work, I thought Iâ€™d do the same with my home PCs.Â Prior to this weekend, Iâ€™ve just had four 1TB SATA drives in a RAID 5 configuration for a disk setup.Â Performance has always been satisfactory, but with recent SSD prices coming down quite a bit, my late 2008 MacBook Pro feeling itâ€™s age, it was time for an upgrade.</description>
    </item>
    
  </channel>
</rss>
